---
title: "Understanding Logistic Regression"
author: [Josep Puig]
output:
  html_document:
date: '2020-04-04'
slug: excellent
categories:
  - Tutorials
  - R
tags: ["R", "Tutorials"]
image:
  caption: ''
  focal_point: ''
summary: "Understanding the insides of logistic regression "
linktitle: "Models"
menu:
  package:
    weight: 30
type: docs
weight: 30
---

<style type="text/css">
.kable-table table {
  margin-left: 0;
}
img {
  border: none;
}
</style>
  
```{r chunk_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6.5,
  fig.height = 3
)
if (capabilities("cairo")) {
  knitr::opts_chunk$set(
    dev.args = list(png = list(type = "cairo"))
  )
}
```

In this post, I would like to explain the estimation of the likelihood function for the GLM models. I will show an example about the logistic regression, but It can be extend to any GLM family (Poisson, Gamma, Gausian,.)

Let me first review some of the basics about logistic regression and the parameters estimation.

Suppose a model where our depend variable $(y_i)$ takes values 0 or 1 (binary response) which could be an indicator about whether a person has not repaid a loan ($y=1$) or not ($y=0$) in the financial field. or, for example, whether a customer has bought a product in marketing.
And also, we have independent variables or predictors, which we will use to estimate a probability of getting $y_i = 1$. 


For each observation, mathematically can be express as:
$$ y_{i} \sim binom(p_{i})$$
Where, 
$$ \eta_{i} = \beta_{0} + X  \beta^{T} + \epsilon_{i} $$
And becasue of $\eta_{i}$ cannot be interpretated as a probability because it can take values from $-\infty$ to $+ \infty$, we must do a "transformation" in order to have the probability properties and in case of the logistic regression we will use the inverse logic function, which is:
$$p_{i} = \frac{1}{1 + exp(-\eta_{i})}$$
Note that $p$ now is between 0 and 1.

To estimate the parameters, we usually use the likelihood function (from the Bayesian point of view, for example, they introduce priors and they use other methods to estimate those paraments).

The likelihood function could be define as: given a predictors and parameters how is likely to observe the data. Therefore, we will have to estimate the parameters that maximize the likelihood function. That is:

$$ \mathcal{L} (y_{i} | x_{1i}, x_{2i}, \dots , x_{pi} , \beta_{0}, \beta_{1},\dots \beta_{p} ) = \prod_{i=1}^{N} p_{i}^{y_{i}} (1-p_{i})^{(1 - y_{i})} $$

For mathematical convenience, we will take the logarithm:

$$ log(\mathcal{L}) = \sum_{i=1}^{N} y_{i} log(p_{i}) + (1 - y_{i}) log(1 - p_{i})$$
Where again, 
$$p_{i} = \frac{1}{1 + exp( - (\beta_{0} + X  \beta^{T} ))}$$
If you try to estimate the parameters by deriving $\mathcal{L}$ respect to beta and make equal to zero, you will not be successful. Therefore, we will need a numerical method to maximize such function.


I propouse the **newthon method** which use Taylor expansion to approximate a function to 0. 

For the optimization of a function with one variable, we have the next procedure:
$$x_{t+1} = x_{t} - \frac{f'(x_{t})}{f''(x_{t}))}$$
We initialized with some value for $x_{t}$ and with this value we compute $x_{t+1}$. The with that value, we again compute $x_{t+2}$ by  $x_{t+2} = x_{t+1} + \frac{f'(x_{t+1})}{f''(x_{t+1}))}$. And iteratively, we will find a $t + k$ until $x_{t+k} = x_{t+k+1}$. We will say that it has **converged**.
Let's see an easy example:

We want to estimate the maximum of:
$$ f(x) = -3x^{4} - 4x+5$$
```{r, message=FALSE, warning=FALSE}
require(ggplot2)
require(dplyr)
fn = function(x){
  -3*x^4 - 4*x + 5
}

ggplot(data = data.frame(x = 0), aes(x)) + 
  stat_function(fun = fn) + xlim(-5,5) + 
  ggtitle('Curve') + theme_minimal()
```


By using the numerical method:
$$x_{t+1} = x_{t} - \frac{f'(x_{t})}{f''(x_{t}))} =  x_{t} - \frac{  12x_{t}^{3} - 4  }{  36  x_{t}^{2}}$$

In R, we define the first and second derivative:
```{r}
fn1 = function(x){
  12*x^3 - 4
}
fn2 = function(x){
  36*x^2
}

```


And we will do the maximum. We initialize by $x=1$ and we will do 8 iterations:

```{r, results='asis'}
require(data.table)
#initalize x
x0 = 1
iter = 8

results = matrix(NA, nrow=iter, ncol=3)
for(i in 1:iter){
  x1 = x0 - fn1(x0)/fn2(x0)
  results[i,] = c(i, x0, x1)
  # x0 will be equal to x1
  x0 = x1
}

results = as.data.table(results)
names(results) = c('iteration','x0','x1')
results %>% 
  melt(., id = 'iteration') %>% 
  ggplot(aes(x = iteration, y = value, color = variable)) + 
  geom_line(size = 1) + labs(x = 'Iteration', y = '', color = '') +
  theme_minimal()

```

Finally, $x_{t+k}$ is equally to `r x1`. Which it has converged because $x_{t+k-1}$ = $x_{t+k}$ 


Moreover, we can check that is the same value as we would compute the derivative and make equal to 0.


$$\frac{\partial f(x)}{\partial x} =  12x^3 - 4 = 0$$
Where,

$$
x = (4/12)^{\frac{1}{3}} \approx 0,6933613
$$


With more parameters to estimate, we will need to extend the initial formula by using matrix and vectors: thereby, $x_{t}$ will become a vector, $ f'(x_{t}) $ will be a vector of the derivatives also called the **gradient**, and $f''(x_{t})$ will become a matrix which diagonal there will be the second derivatives and rest of the elements would be the partial ones. This also is called as the **Hessian** matrix.

$$\vec{x_{t+1}} = \vec{x_{t}} - \alpha H( \vec{x_{t}})^{-1} gradient(\vec{x_{t}})$$

$\alpha$ $\ni$ $[0,1]$ is a parameter which could be define as either the speed to get the convergence or the precision of the parameters. If $\alpha$ is closed to one, the faster will be the approximation to 0 but would be less accurate.


Let's make an example with R.

We will simulate a sample with two predictors:

```{r}
set.seed(100)
x1 = rnorm(3000)
x2 = rnorm(3000)
eta = -1.2 -1.1*x1 + 1.2*x2 + rnorm(3000)
p = 1/(1+exp(-eta))
y = rbinom(3000, 1, p)
table(y)
```

With the **glm** function we will estimate easily the parameters:

```{r, results='asis'}
mod = glm(y~x1+x2, family = binomial())
stargazer::stargazer(mod)
```

Now, we will estimate the parameters using the method I describe above. I will use the *NumDeriv* package, that easily, it calcules the gradient and Hessian matrix given a function:


```{r}
# First we define our X matrix. The first column is the intercept
X = cbind(1,x1,x2)


Likelihood = function(par){
  par = matrix(par, nrow=1)
  eta = X %*% t(par)
  p = 1/(1 + exp(-eta))
  # Likelihood function:
  return( sum ( y * log(p) + (1-y) * log(1 - p)) )
}
```

Once we have defined the likelihood function, we will do the estimation process:

```{r, results='asis'}
require(numDeriv)
# Number of iterations
iters = 20
results = matrix(NA, nrow=iters+1, ncol=5)
x0 = rep(.5,3)
# Define alpha
alpha = .5
results[1,] = c(0, x0, Likelihood(x0))
for(i in 1:iters){
  H = hessian(func = Likelihood, x0)
  gradient = matrix(grad(Likelihood, x0), ncol=1, nrow=length(x0))
  x1 = as.numeric(x0 - alpha * solve(H) %*% gradient)
  results[i+1, ] = c(i, x1, Likelihood(x1))
  x0 = x1
}

results = as.data.table(results)
names(results) = c('Iteration','Intercept','x1','x2','Likelihood')
results

```


Note that the coefficients are the same as the direct way that we did before. And also the likelihood value is the same:

```{r}
print(logLik(mod))
print(mod$coefficients)
print(results[nrow(results),])

```


```{r}
results %>% 
  dplyr::select('Iteration','Intercept','x1','x2') %>% 
  melt(., id = 'Iteration') %>% 
  ggplot(aes(x = Iteration, y = value, color = variable)) + 
  geom_line(size = 1) + labs(x = 'Iteration', y = '', color = '') +
  theme_minimal() + 
  facet_wrap(~ variable, scales = 'free')
```

